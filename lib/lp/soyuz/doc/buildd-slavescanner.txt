= Buildd Slave Scanner =

The Buildd Slave scanner is able to run over the build jobs being
processed in the current BuildFarm and collect information about the
status of the process, collect the results of finished jobs and
automatically dispatch new jobs to idle slaves.

The Master side of Buildd requires access to Launchpad Database, the
user designed for this kind of access is 'fiera', as in all test the
transaction should be retrieved.

    >>> from canonical.database.sqlbase import ZopelessTransactionManager
    >>> local_transaction = ZopelessTransactionManager._installed

We check for sent mails in some places, so load the stub mailer:

    >>> from lp.services.mail import stub
    >>> from canonical.database.sqlbase import commit

And create a utility function to make tests easier to read.

    >>> def check_mail_sent(last_stub_mail_count):
    ...    commit()
    ...    return len(stub.test_emails) == last_stub_mail_count + 3

The master also requires an 'logging' instance to not compromise the
standard output with noisily output.

    >>> import logging
    >>> logger = logging.getLogger()

Import MockBuilder and a series of MockSlaves to be used in this test.

    >>> from lp.soyuz.tests.soyuzbuilddhelpers import (
    ...    AbortedSlave, AbortingSlave, BuildingSlave,
    ...    LostBuildingBrokenSlave, MockBuilder, OkSlave, WaitingSlave)

Slave-scanner will deactivate a 'lost-building' builder that could not
be aborted appropriately.

    >>> from zope.security.proxy import removeSecurityProxy
    >>> from lp.buildmaster.interfaces.builder import CorruptBuildCookie
    >>> from lp.testing.fakemethod import FakeMethod
    >>> lostbuilding_builder = MockBuilder(
    ...     'Lost Building Broken Slave', LostBuildingBrokenSlave())
    >>> behavior = removeSecurityProxy(
    ...     lostbuilding_builder.current_build_behavior)
    >>> behavior.verifySlaveBuildCookie = FakeMethod(
    ...     failure=CorruptBuildCookie("Hopelessly lost!"))

    >>> lostbuilding_builder.updateStatus(logger)
    Aborting slave
    WARNING:root:Lost Building Broken Slave (http://fake:0000) marked as failed due to: <Fault 8002: 'Could not abort'>
    Traceback (most recent call last):
    ...
    Fault: <Fault 8002: 'Could not abort'>

'ensurePresent()' slave method always return True, it theoretically
means the slave has the requested file in cache.  In our MockBuilder
we simply display the URL of the file we're asked to get from the
librarian.  Typically the first file is always the chroot, which in
the case of this doctest is a dummy alias pointing at netapplet (!) so
it is not shown in each case below.

The mock slaves will also print, when necessary, whether it has been
passed an 'archives' property in the args dictionary.

The archives are passed from the buildmaster and controls what archives
exist in the apt sources.list.  If nothing is passed, the chroot's default
list applies, otherwise the passed list is used.  This behavior is required
in build slaves because some jobs may only depend on certain archives and
hence certain package dependencies.

The slavescanner system also perform build-notification for the
following states: FAILEDTOBUILD and CHROOTWAIT

    >>> from lp.buildmaster.interfaces.builder import IBuilderSet
    >>> from lp.soyuz.interfaces.binarypackagebuild import (
    ...     IBinaryPackageBuildSet)
    >>> import datetime, pytz

    >>> UTC = pytz.timezone('UTC')

We want to get a Build and make BuildQueue items for it:

    >>> a_build = getUtility(IBinaryPackageBuildSet).getByBuildID(8)

To make testing easier we provide a convenience function to put a BuildQueue
object into a preset fixed state:

    >>> default_start = datetime.datetime(2005, 1, 1, 8, 0, 0, tzinfo=UTC)
    >>> def setupBuildQueue(build_queue, builder):
    ...     build_queue.markAsBuilding(builder)

Remove any previous buildmaster ROOT directory, to avoid any garbage
lock conflict (it would be recreated automatically if necessary)

    >>> from canonical.config import config
    >>> import shutil
    >>> import os
    >>> if os.access(config.builddmaster.root, os.F_OK):
    ...     shutil.rmtree(config.builddmaster.root)

Let's check the procedures to verify/collect running build process:

  WAITING - PACKAGEFAIL -> Package has failed to build, notice from
  builder is stored, but Build.status is mark as 'Failed to Build':

Get a builder from the sample data:

    >>> a_builder = getUtility(IBuilderSet).get(1)

Make sure that a_builder has no active builds:

    >>> from canonical.launchpad.ftests import syncUpdate
    >>> if a_builder.currentjob is not None:
    ...     currentjob = a_builder.currentjob
    ...     currentjob.setDateStarted(None)
    ...     currentjob.builder = None
    ...     syncUpdate(currentjob)

Force the test builder to be 'ok' as the code required to do this
automatically is not yet factored into the content class.

    >>> a_builder.builderok = True

Create a mock slave so the builder can operate - one with a failed package.

    >>> a_builder.setSlaveForTesting(WaitingSlave('BuildStatus.PACKAGEFAIL'))

    >>> bqItem3 = a_build.buildqueue_record
    >>> setupBuildQueue(bqItem3, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Do the test execution:

    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem3)
    >>> a_builder.updateBuild(bqItem3)
    >>> build.builder is not None
    True
    >>> build.date_finished is not None
    True
    >>> build.duration is not None
    True
    >>> build.log is not None
    True
    >>> check_mail_sent(last_stub_mail_count)
    True
    >>> build.status.title
    'Failed to build'

WAITING - DEPWAIT -> a required dependency is missing, again notice
from builder, but Build.status has the right state:

    >>> bqItem4 = a_build.queueBuild()
    >>> setupBuildQueue(bqItem4, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder can operate - one with a dependency error.

    >>> bqItem4.builder.setSlaveForTesting(
    ...                        WaitingSlave('BuildStatus.DEPFAIL',
    ...                                     'baz (>= 1.0.1)'))

Do the test execution:

    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem4)
    >>> a_builder.updateBuild(bqItem4)
    CRITICAL:slave-scanner:***** bob is MANUALDEPWAIT *****
    >>> build.builder is not None
    True
    >>> build.date_finished is not None
    True
    >>> build.duration is not None
    True
    >>> build.log is not None
    True
    >>> check_mail_sent(last_stub_mail_count)
    False
    >>> build.dependencies
    u'baz (>= 1.0.1)'
    >>> build.status.title
    'Dependency wait'

WAITING - CHROOTFAIL -> the Chroot for this distroseries is damage, nor
builder, but right state stored in Build entry:

    >>> bqItem5 = a_build.queueBuild()
    >>> setupBuildQueue(bqItem5, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

  Create a mock slave so the builder can operate - one with a failed chroot.

    >>> bqItem5.builder.setSlaveForTesting(
    ...     WaitingSlave('BuildStatus.CHROOTFAIL'))
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem5)
    >>> a_builder.updateBuild(bqItem5)
    CRITICAL:slave-scanner:***** bob is CHROOTWAIT *****
    >>> build.builder is not None
    True
    >>> build.date_finished is not None
    True
    >>> build.duration is not None
    True
    >>> build.log is not None
    True
    >>> check_mail_sent(last_stub_mail_count)
    True
    >>> build.status.title
    'Chroot problem'

WAITING - BUILDERFAIL -> builder has failed by internal error, job is available for next build round:

    >>> bqItem6 = a_build.queueBuild()
    >>> bqItem6.markAsBuilding(a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder can operate - one with a builder error.

    >>> bqItem6.builder.setSlaveForTesting(
    ...     WaitingSlave('BuildStatus.BUILDERFAIL'))

    >>> a_builder.updateBuild(bqItem6)
    WARNING:slave-scanner:***** bob has failed *****

    >>> from canonical.launchpad.ftests import sync
    >>> sync(a_builder)
    >>> a_builder.failnotes
    u'Builder returned BUILDERFAIL when asked for its status'

    >>> bqItem6.builder is None
    True
    >>> check_mail_sent(last_stub_mail_count)
    False
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem6)
    >>> print build.status.title
    Needs building
    >>> job = bqItem6.specific_job.job
    >>> print job.status.title
    Waiting

Cleanup in preparation for the next test:

    >>> bqItem6.destroySelf()
    >>> a_builder.builderok = True


BUILDING -> builder still processing the job, simply collect the logtail:

    >>> bqItem7 = a_build.queueBuild()
    >>> setupBuildQueue(bqItem7, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder can operate - one which is building.

    >>> bqItem7.builder.setSlaveForTesting(BuildingSlave())
    >>> builder_id = bqItem7.builder.id
    >>> a_builder.updateBuild(bqItem7)

Due to updateBuild doing a commit we cannot compare the object instance.

    >>> bqItem7.builder.id is builder_id
    True
    >>> check_mail_sent(last_stub_mail_count)
    False
    >>> bqItem7.logtail
    u'This is a build log'

Cleanup in preparation for the next test:

    >>> bqItem7.destroySelf()

ABORTED -> builder was aborted, release builder and reset job for the
next build round:

    >>> bqItem8 = a_build.queueBuild()
    >>> setupBuildQueue(bqItem8, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

    >>> bqItem8.builder.setSlaveForTesting(BuildingSlave())
    >>> a_builder.updateBuild(bqItem8)
    >>> bqItem8.builder.setSlaveForTesting(AbortedSlave())
    >>> bqItem8.builder.name
    u'bob'
    >>> a_builder.updateBuild(bqItem8)
    >>> bqItem8.builder is None
    True
    >>> print bqItem8.specific_job.build.status.name
    NEEDSBUILD

Cleanup in preparation for the next test:

    >>> bqItem8.destroySelf()

ABORTING -> builder is trying to terminate its children process, the
only action master can perform is polling the slave status until it
gets ABORTED.

    >>> bqItem9 = a_build.queueBuild()
    >>> setupBuildQueue(bqItem9, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

    >>> bqItem9.builder.setSlaveForTesting(AbortingSlave())
    >>> bqItem9.builder.name
    u'bob'
    >>> a_builder.updateBuild(bqItem9)
    >>> check_mail_sent(last_stub_mail_count)
    False
    >>> bqItem9.logtail
    u'Waiting for slave process to be terminated'

Cleanup in preparation for the next test:

    >>> bqItem9.destroySelf()


== Builder WAITING in OK state ==

This situation happens when the builder has finished the job and is
waiting for the master to collect its results.

The build record in question can end up in the following states:

 * FULLYBUILT: when binaries were collected and uploaded correctly;
 * FAILEDTOUPLOAD: binaries were collected but the upload was
                   rejected/failed.


=== Failed to Upload (FAILEDTOUPLOAD) ===

    >>> bqItem10 = a_build.queueBuild()
    >>> setupBuildQueue(bqItem10, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder gets the right responses for this test.

    >>> bqItem10.builder.setSlaveForTesting(
    ...     WaitingSlave('BuildStatus.OK'))

If the build record wasn't updated before/during the updateBuild
(precisely on binary upload time), the build will be considered
FAILEDTOUPLOAD:

    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem10)
    >>> a_builder.updateBuild(bqItem10)
    WARNING:slave-scanner:Build ... upload failed.
    >>> build.builder is not None
    True
    >>> build.date_finished is not None
    True
    >>> build.duration is not None
    True
    >>> build.log is not None
    True
    >>> check_mail_sent(last_stub_mail_count)
    True
    >>> build.status.title
    'Failed to upload'

Let's check the emails generated by this 'failure'
(see build-failedtoupload-workflow.txt for more information):

    >>> from operator import itemgetter
    >>> local_test_emails = stub.test_emails[last_stub_mail_count:]
    >>> local_test_emails.sort(key=itemgetter(1), reverse=True)
    >>> for from_addr, to_addrs, raw_msg in local_test_emails:
    ...      print to_addrs
    ['mark@example.com']
    ['foo.bar@canonical.com']
    ['celso.providelo@canonical.com']

Note that a real failed-to-upload notification contains the respective
upload log information:

    >>> one_email = stub.test_emails.pop()
    >>> from_addr, to_addrs, raw_msg = one_email
    >>> print raw_msg
    Content-Type: text/plain; charset="utf-8"
    ...
    X-Launchpad-Build-State: FAILEDTOUPLOAD
    ...
    * Build Log: http://.../...i386.mozilla-firefox_0.9_BUILDING.txt.gz
    ...
    Upload log:
    INFO    Creating lockfile:...
    DEBUG   Initialising connection.
    ...
    DEBUG   Removing lock file: /var/lock/process-upload-buildd.lock
    ...

When a failure in processing the generated binaries occurs, the log
output is both emailed in an immediate notification, and stored in the
librarian for future reference.

    >>> build.upload_log is not None
    True

What we can clearly notice is that the log is still containing
the old build state (BUILDING) in its name. This is a minor problem
that can be sorted by modifying the execution order of procedures
inside Buildergroup.buildStatus_OK method.


=== Successfully collected and uploaded  (FULLYBUILT) ===

Build item 6 has binary packages available in the sample data, letting us test
this case cleanly. We need to set the pocket to updates for this test as its
uploading to warty.

    >>> bqItem10 = getUtility(IBinaryPackageBuildSet).getByBuildID(
    ...     6).queueBuild()
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem10)

XXX: The pocket attribute is not intended to be changed in regular code, but
for this test we want to change it on the fly. An alternative would be to add
new sample data for a build that can be uploaded with binary packages attached
to it.

    >>> from lp.registry.interfaces.pocket import PackagePublishingPocket
    >>> removeSecurityProxy(
    ...     build).pocket = PackagePublishingPocket.UPDATES
    >>> setupBuildQueue(bqItem10, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder gets the right responses for this test.

    >>> bqItem10.builder.setSlaveForTesting(WaitingSlave('BuildStatus.OK'))

Now in order to emulate a successfully binary upload we will update
the build record to FULLYBUILT, as the process-upload would do:

    >>> from lp.buildmaster.interfaces.buildbase import BuildStatus
    >>> build.status = BuildStatus.FULLYBUILT

Now the updateBuild should recognize this build record as a
Successfully built and uploaded procedure, not sending any
notification and updating the build information:

    >>> a_builder.updateBuild(bqItem10)
    >>> build.builder is not None
    True
    >>> build.date_finished is not None
    True
    >>> build.duration is not None
    True
    >>> build.log is not None
    True
    >>> build.status.title
    'Successfully built'
    >>> check_mail_sent(last_stub_mail_count)
    False

We do not store any build log information when the binary upload
processing succeeded.

    >>> build.upload_log is None
    True

WAITING -> GIVENBACK - slave requested build record to be rescheduled.

    >>> bqItem11 = a_build.queueBuild()
    >>> bqItem11.markAsBuilding(a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder gets the right responses for this test.

    >>> bqItem11.builder.setSlaveForTesting(
    ...     WaitingSlave('BuildStatus.GIVENBACK'))
    >>> a_builder.updateBuild(bqItem11)
    WARNING:slave-scanner:***** i386 build of mozilla-firefox 0.9 in ubuntu hoary RELEASE is GIVENBACK by bob *****

Ensure GIVENBACK build preserves the history for future use. (we
can't be sure if logtail will contain any information, because it
depends on how long the build took to be processed and how often we
scanned it)

    >>> bqItem11.builder is None
    True
    >>> bqItem11.date_started is None
    True
    >>> bqItem11.lastscore
    2505
    >>> check_mail_sent(last_stub_mail_count)
    False
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem11)
    >>> print build.status.title
    Needs building
    >>> job = bqItem11.specific_job.job
    >>> print job.status.title
    Waiting

Cleanup in preparation for the next test:

    >>> bqItem11.destroySelf()

The Builddmaster should crash when collecting builds which are denied in
the given distroseries/pocket. Anytime it happens we need to manually
investigate why this build end up built. (should never happen in real
cases, and even so should be refused when we try to upload it.)

    >>> bqItem12 = getUtility(IBinaryPackageBuildSet).getByBuildID(
    ...     2).queueBuild()
    >>> setupBuildQueue(bqItem12, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Create a mock slave so the builder gets the right responses for this test.

    >>> bqItem12.builder.setSlaveForTesting(WaitingSlave('BuildStatus.OK'))
    >>> a_builder.updateBuild(bqItem12)
    Traceback (most recent call last):
    ...
    AssertionError: i386 build of mozilla-firefox 0.9 in ubuntu warty RELEASE (2) can not be built for pocket RELEASE: illegal status

We need 'a_builder' released (from 'bqItem12') so it can be associated with
'bqItem10' below.

    >>> bqItem12.builder = None

The log is collected and compressed locally using gzip algorithm,
let's see how this method works:

    >>> bqItem10 = getUtility(IBinaryPackageBuildSet).getByBuildID(
    ...     6).queueBuild()
    >>> setupBuildQueue(bqItem10, a_builder)
    >>> build = bqItem10.specific_job.build
    >>> build.status = BuildStatus.FULLYBUILT
    >>> bqItem10.builder.setSlaveForTesting(WaitingSlave('BuildStatus.OK'))

Before collecting and processing the log we will store the files
already created in /tmp so we can verify later that this mechanism is
not leaving any temporary file behind. See bug #172798.

    >>> old_tmps = os.listdir('/tmp')

Collect and process the log.

    >>> logfile_alias = build.getLogFromSlave(build)

Audit the /tmp for lost temporary files, there should not be any new
files. For the record, the procedure creates files with the
'.buildlog' suffix.

    >>> sorted(os.listdir('/tmp')) == sorted(old_tmps)
    True

The log was compressed and directly transferred to Librarian.

    >>> from canonical.launchpad.interfaces import ILibraryFileAliasSet
    >>> logfile = getUtility(ILibraryFileAliasSet)[logfile_alias]

    >>> print logfile.filename
    buildlog_ubuntu-warty-i386.foobar_1.0_FULLYBUILT.txt.gz

    >>> print logfile.mimetype
    text/plain

Needed so that the Librarian can serve the new file.

    >>> commit()

Check if the log content is correct and accessible via the
library file directly and via Librarian http front-end.

Since LibrarianFileAlias does not implement required attributes for
gzip.open() (like tell() or seek()) we are obligated to read it again
in our filesystem.

    >>> built_builder = MockBuilder(
    ...     'Package Successfully Built',
    ...      WaitingSlave('BuildStatus.OK'))

    >>> import gzip, tempfile
    >>> fd, fname = tempfile.mkstemp()
    >>> tmp = os.fdopen(fd, 'wb')
    >>> tmp.write(logfile.read())
    >>> tmp.close()
    >>> gzip.open(fname).read() == built_builder.slave.getFile('buildlog').read()
    True

This also happens with urllib instance, we need to download it to the
filesystem before decompress.

    >>> import urllib
    >>> from_web = urllib.urlopen(logfile.http_url)
    >>> tmp = open(fname, 'wb')
    >>> tmp.write(from_web.read())
    >>> tmp.close()
    >>> gzip.open(fname).read() == built_builder.slave.getFile('buildlog').read()
    True

Both access methods work as expected, remove the temporary file used here.

    >>> os.remove(fname)

The Librarian serves log files with 'gzip' content-encoding and
'text/plain' content-type. This combination instructs the browser to
decompress the file and display it inline, which makes it easier for
users to view it.

    >>> from canonical.launchpad.webapp.url import urlparse
    >>> parsed_url = urlparse(logfile.http_url)
    >>> netloc, path = parsed_url[1:3]

    >>> import httplib
    >>> con = httplib.HTTPConnection(netloc)
    >>> con.request("HEAD", path)
    >>> resp = con.getresponse()
    >>> headers = dict(resp.getheaders())

    >>> print headers['content-encoding']
    gzip

    >>> print headers['content-type']
    text/plain

Check the log from the uploader run has made it into the upload directory:

    >>> failed_dir = os.path.join(config.builddmaster.root, 'failed')
    >>> failed_uploads = sorted(os.listdir(failed_dir))
    >>> len(failed_uploads)
    2

    >>> failed_upload = failed_uploads[0]
    >>> uploader_log = open(os.path.join(failed_dir, failed_upload,
    ...                                  'uploader.log'))

    >>> print uploader_log.read()
    INFO    Creating lockfile:...
    DEBUG   Initialising connection.
    DEBUG   Beginning processing
    DEBUG   Creating directory /var/tmp/builddmaster/accepted
    DEBUG   Creating directory /var/tmp/builddmaster/rejected
    DEBUG   Creating directory /var/tmp/builddmaster/failed
    ...
    DEBUG   Rolling back any remaining transactions.
    DEBUG   Removing lock file: /var/lock/process-upload-buildd.lock
    <BLANKLINE>

Remove build upload results root

    >>> shutil.rmtree(config.builddmaster.root)
    >>> bqItem10.destroySelf()
    >>> setupBuildQueue(bqItem12, a_builder)


== Setup chroots ==

Retrieve a known DistroArchSeries

    >>> from canonical.launchpad.interfaces import IDistributionSet
    >>> hoary_i386 = getUtility(IDistributionSet)['ubuntu']['hoary']['i386']
    >>> warty_i386 = getUtility(IDistributionSet)['ubuntu']['warty']['i386']

Create a totally bogus CHROOT

    >>> from canonical.launchpad.database import LibraryFileAlias
    >>> fake_chroot = LibraryFileAlias.get(1)
    >>> unused = hoary_i386.addOrUpdateChroot(fake_chroot)
    >>> unused = warty_i386.addOrUpdateChroot(fake_chroot)


== Build Dispatching ==

Build dispatching can be entirely done via IBuilder content class
using the findAndStartJob method.

We will use SoyuzTestPublisher to simulate the required context in the
next tests. Let's initialise it.

    >>> from lp.soyuz.tests.test_publishing import (
    ...     SoyuzTestPublisher)
    >>> from canonical.testing.layers import LaunchpadZopelessLayer

    >>> test_publisher = SoyuzTestPublisher()

    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')

    >>> test_publisher.prepareBreezyAutotest()

    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

Helper function to create binary publications in this test.

    >>> def create_binary_publication_for(archive, distroseries, status):
    ...     commit()
    ...     LaunchpadZopelessLayer.switchDbUser('launchpad')
    ...     login('foo.bar@canonical.com')
    ...     pub_binaries = test_publisher.getPubBinaries(
    ...         archive=archive, distroseries=distroseries,
    ...         status=status)
    ...     commit()
    ...     LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)
    ...     login(ANONYMOUS)

We will reset the sampledata building job before continue with the
tests.

    >>> current_job = a_builder.currentjob
    >>> resurrect_build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(
    ...     current_job)
    >>> resurrect_build.status = BuildStatus.NEEDSBUILD
    >>> syncUpdate(resurrect_build)
    >>> current_job.builder = None
    >>> current_job.setDateStarted(None)
    >>> current_job.lastscore = 0
    >>> syncUpdate(current_job)

IBuilder.findCandidate also identifies if there are builds for
superseded source package releases in the queue and marks the
corresponding build record as SUPERSEDED.

    >>> old_candidate = removeSecurityProxy(a_builder)._findBuildCandidate()
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(
    ...     old_candidate)
    >>> print build.status.name
    NEEDSBUILD

The 'candidate' is constant until we dispatch it.

    >>> new_candidate = removeSecurityProxy(a_builder)._findBuildCandidate()
    >>> new_candidate.id == old_candidate.id
    True

Now let's disable the archive of the associated build record and see
whether the candidate will still be found.

    >>> build.archive.disable()
    >>> new_candidate = removeSecurityProxy(a_builder)._findBuildCandidate()
    >>> new_candidate is None
    True

The build candidate was not found because builds associated with disabled
archives are ignored. Now let's re-enable that archive and the build
candidate will be found again.

    >>> build.archive.enable()
    >>> new_candidate = removeSecurityProxy(a_builder)._findBuildCandidate()
    >>> new_candidate.id == old_candidate.id
    True

In order to make the current candidate be considered 'superseded' we
need to tweak the status of the current publication directly, as a
permissive database user.

    >>> from canonical.config import config
    >>> from canonical.launchpad.interfaces import PackagePublishingStatus
    >>> from canonical.testing.layers import LaunchpadZopelessLayer

    >>> spr = build.source_package_release
    >>> pub = removeSecurityProxy(build).current_source_publication
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> pub.status = PackagePublishingStatus.SUPERSEDED
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

Now, there we have another build candidate.

    >>> new_candidate = removeSecurityProxy(a_builder)._findBuildCandidate()
    >>> new_candidate.id != old_candidate.id
    True

Because the 'previous' candidate was marked as superseded, so it's not
part of the candidates list anymore.

    >>> print build.status.name
    SUPERSEDED

If the candidate is for a private build whose source has not been
published yet, it will be temporarily skipped until the source is
published.  We need to tweak the status of the publishing record again
to demonstrate this, and also make the archive private:

XXX Michael Nelson 2010-02-19 bug=394276 Please let's put some time
aside to convert these to unit-tests.

    >>> naked_build = removeSecurityProxy(
    ...     getUtility(IBinaryPackageBuildSet).getByQueueEntry(new_candidate))
    >>> original_archive = naked_build.archive
    >>> secure_pub = naked_build.current_source_publication
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> private_ppa = factory.makeArchive(private=True)
    >>> naked_build.archive = private_ppa
    >>> secure_pub.archive = private_ppa
    >>> secure_pub.status = PackagePublishingStatus.PENDING
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

Let's try to find a new build candidate:

    >>> another_candidate = removeSecurityProxy(
    ...     a_builder)._findBuildCandidate()

Since there are no more candidates at all, _findBuildCandidate()
returned None:

    >>> print another_candidate
    None

If we publish the source, the build candidate will be found again:

    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> secure_pub.status = PackagePublishingStatus.PUBLISHED
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

    >>> another_candidate = removeSecurityProxy(
    ...     a_builder)._findBuildCandidate()
    >>> another_candidate.id == new_candidate.id
    True

If the source is subsequently deleted or superseded before the build
starts it is also returned as a candidate so that the build can be
superseded.  We can supersede this publication which will have the effect of
making the build be superseded and no candidate is returned.

    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> secure_pub = naked_build.current_source_publication
    >>> secure_pub.status = PackagePublishingStatus.DELETED
    >>> secure_pub.status = PackagePublishingStatus.SUPERSEDED
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(
    ...     current_job)
    >>> print build.status.name
    NEEDSBUILD

    >>> another_candidate = removeSecurityProxy(
    ...     a_builder)._findBuildCandidate()
    >>> print another_candidate
    None

    >>> print build.status.name
    SUPERSEDED

We'll reset the archive back to non-private for further tests:

    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> naked_build.archive = original_archive
    >>> secure_pub.archive = original_archive
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

For building a candidate in the release pocket for the main component
and the primary archive It will pass an 'archives' argument to the
slave that contains sources.list entries for each pocket required in
the primary archive dependency tree.

We also pass arguments called 'suite' which is the current distroseries and
pocket, (e.g. edgy-updates) and 'archive_purpose' which contains the build's
archive.purpose (e.g. PRIMARY or PPA).  These latter two arguments are
used in the chroot to determine whether it needs to turn on some features
or not (like pkgstriptranslations and pkgmaintainermangler).

Please note also that the 'archive_private' flag is passed to the slave
builder.  It is True for private archives and False otherwise.

    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True
    >>> candidate = a_build.queueBuild()
    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main
    Suite: hoary
    Ogre-component: main
    Archive Purpose: PRIMARY
    Archive Private: False

    >>> candidate.destroySelf()

Currently we can theoretically dispatch a build candidate for a
builder in 'manual' mode.

Although this will not be optimal, because we can only
do it once the manual builder has been collected (due to the
BuildQueue.builder constraint). Also because we don't yet provide a
API/UI method to request the dispatch in advance.

    >>> a_builder.manual = True
    >>> commit()
    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True
    >>> candidate = a_build.queueBuild()
    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main
    Suite: hoary
    Ogre-component: main
    Archive Purpose: PRIMARY
    Archive Private: False

    >>> candidate.destroySelf()

Partner archive builds will set up the 'archives' argument such that it
references all the required pockets/components in the primary archive, in
addition to a reference to the release pocket in the partner archive itself.

    >>> ubuntu = getUtility(IDistributionSet)['ubuntu']
    >>> partner_archive = ubuntu.getArchiveByComponent('partner')
    >>> removeSecurityProxy(a_build).archive = partner_archive
    >>> commit()
    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

The partner archive won't be passed to the builder unless it has at
least one published binary availble in the target distroarchseries.
This feature fixes bug #196782, when archive/suites got passed to
builders before they get published on disk, i.e. the first build on
any PPA/suite will fail during the first 20 minutes because no empty
indexes are published.

Note that only a published binary in the right context will make the
archive relevant, anything PENDING or published in another context
wouldn't work.

    >>> warty = getUtility(IDistributionSet)['ubuntu']['warty']
    >>> create_binary_publication_for(
    ...    partner_archive, warty, PackagePublishingStatus.PUBLISHED)

    >>> hoary = getUtility(IDistributionSet)['ubuntu']['hoary']
    >>> create_binary_publication_for(
    ...    partner_archive, hoary, PackagePublishingStatus.PENDING)

So, at moment, partner archive is still not relevant for builds in
hoary/i386. It's not passed to the builder.

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates main restricted universe multiverse
    Suite: hoary
    Ogre-component: main
    Archive Purpose: PARTNER
    Archive Private: False

Let's try it again.

    >>> candidate.destroySelf()
    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

    >>> removeSecurityProxy(a_build).archive = ubuntu.main_archive
    >>> candidate.destroySelf()

But this time We will create a valid publication on partner hoary/i386.

    >>> from lp.soyuz.interfaces.component import IComponentSet
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> login('foo.bar@canonical.com')
    >>> pub_source = test_publisher.getPubSource(
    ...    archive=partner_archive, distroseries=hoary,
    ...    status=PackagePublishingStatus.PUBLISHED,
    ...    component='partner')
    >>> pub_binaries = test_publisher.getPubBinaries(
    ...     archive=partner_archive, pub_source=pub_source,
    ...     distroseries=hoary, status=PackagePublishingStatus.PUBLISHED)
    >>> partner_build = pub_binaries[0].binarypackagerelease.build
    >>> partner_candidate = partner_build.buildqueue_record
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)

Now when we dispatch the partner build, since it has one published
binary in hoary/i386, the partner archive gets included in the builder
sources_list.

    >>> removeSecurityProxy(
    ...     a_builder)._dispatchBuildCandidate(partner_candidate)
    ensurepresent called, url=...
    ensurepresent called, url=http://localhost:58000/.../foo_666.dsc
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary
         main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security
         main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates
         main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu-partner hoary partner
    Suite: hoary
    Ogre-component: partner
    Archive Purpose: PARTNER
    Archive Private: False

    >>> partner_candidate.destroySelf()

Similarly, PPA builds pass the 'archives' arguments:

    >>> from canonical.launchpad.interfaces import IPersonSet
    >>> cprov_archive = getUtility(IPersonSet).getByName('cprov').archive
    >>> removeSecurityProxy(a_build).archive = cprov_archive
    >>> a_builder.virtualized = True
    >>> a_builder.vm_host = 'localhost.ppa'
    >>> commit()
    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Exactly as Partner, Celso's PPA won't be included if it doesn't
contain any published binary in hoary/i386. We will create it before
dispatching.

    >>> create_binary_publication_for(
    ...    cprov_archive, hoary, PackagePublishingStatus.PUBLISHED)

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates main restricted universe multiverse
     deb http://ppa.launchpad.dev/cprov/ppa/ubuntu hoary main
    Suite: hoary
    Ogre-component: main
    Archive Purpose: PPA
    Archive Private: False

If the build is for a private PPA, the slave scanner will pass a
sources.list entry that contains a password to access the archive.

    >>> from canonical.testing import LaunchpadZopelessLayer
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> login('foo.bar@canonical.com')
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(
    ...     candidate)
    >>> for build_file in build.source_package_release.files:
    ...     removeSecurityProxy(build_file).libraryfile.restricted = True
    >>> private_ppa = factory.makeArchive(
    ...     owner=cprov_archive.owner, name='pppa', private=True,
    ...     virtualized=False, distribution=ubuntu)

It's necessary to publish some binaries into the private PPA, otherwise
the PPA won't be included as a dependency in the sources list below.

    >>> binaries = test_publisher.getPubBinaries(
    ...     distroseries=ubuntu['hoary'], archive=private_ppa,
    ...     status=PackagePublishingStatus.PUBLISHED)
    >>> removeSecurityProxy(build).archive = private_ppa
    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(test_dbuser)
    >>> login(ANONYMOUS)

Dispatch the build again. Celso's archive sources.list entry now has the
buildd:secret@ part in the URL.

Also note that when ensurepresent() is called, it receives a URL that
points to the private archive rather than the librarian for the private
firefox file.  This is because the build slaves are not allowed to
access the restricted librarian as it cannot provide access via
credentials, unlike the archive itself.

Finally, the archive purpose is overridden to PRIMARY instead of PPA
for any archives that have require_virtualized as False.

In this circumstance, it also uses the component override from the PRIMARY
archive and not the one from the PPA, which on the absence of ancestry
defaults to 'universe'.

    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(candidate)
    >>> print build.current_component.name
    main

This is so that the mangling tools will run over the built packages.

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
     url=http://private-ppa.../cprov/pppa/.../firefox_0.9.2.orig.tar.gz
    URL authorisation with buildd/sekrit
    OkSlave BUILDING
    Archives:
     deb http://buildd:sekrit@private-ppa.../cprov/pppa/ubuntu hoary main
     deb http://ftpmaster.internal/ubuntu hoary
         main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security
         main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates
         main restricted universe multiverse
    Suite: hoary
    Ogre-component: universe
    Archive Purpose: PRIMARY
    Archive Private: True

We will create an ancestry in the primary archive target to the 'main'
component and this time the dispatching will follow that component.

    >>> sourcename = build.source_package_release.name

    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> login('foo.bar@canonical.com')

    >>> ancestry = test_publisher.getPubSource(
    ...     sourcename=sourcename, version='0.1', distroseries=hoary)

    >>> print ancestry.displayname
    mozilla-firefox 0.1 in hoary

    >>> print ancestry.component.name
    main

    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(config.builddmaster.dbuser)
    >>> login(ANONYMOUS)

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, ...
    ...
    Ogre-component: main
    ...

    >>> candidate.destroySelf()

Since this is a build in a private archive, the log was uploaded to
the restricted librarian.

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> build.upload_log = None
    >>> candidate.builder.setSlaveForTesting(WaitingSlave('BuildStatus.OK'))
    >>> a_builder.updateBuild(candidate)
    WARNING:slave-scanner:Build ... upload failed.
    >>> local_transaction.commit()

    >>> build.archive.private
    True

    >>> lfa = build.log
    >>> lfa.restricted
    True
    >>> print lfa.filename
    buildlog_ubuntu-hoary-i386.mozilla-firefox_0.9_BUILDING.txt.gz

The attempt to fetch the buildlog from the common librarian will fail
since this is a build in a private archive and the buildlog was thus
uploaded to the restricted librarian.

    >>> from canonical.librarian.interfaces import ILibrarianClient
    >>> getUtility(ILibrarianClient).getFileByAlias(lfa.id)
    Traceback (most recent call last):
      ...
    DownloadFailed: Alias ... cannot be downloaded from this client.

Accessing the log via the restricted librarian will work as expected.

    >>> import urlparse
    >>> from canonical.librarian.interfaces import IRestrictedLibrarianClient
    >>> lfa2 = removeSecurityProxy(
    ...     getUtility(IRestrictedLibrarianClient).getFileByAlias(lfa.id))
    >>> url_parts = urlparse.urlsplit(lfa2.file.geturl())
    >>> print os.path.basename(url_parts[2])
    buildlog_ubuntu-hoary-i386.mozilla-firefox_0.9_BUILDING.txt.gz

A PPA can depend on another PPA. We can make Celso's PPA depend on
Mark's PPA:

    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser('launchpad')
    >>> login('foo.bar@canonical.com')

We'll switch the build's archive back to Celso's PPA and set the PPA to
virtualized before adding the dependency on Mark's PPA.

    >>> removeSecurityProxy(build).archive = cprov_archive
    >>> cprov_archive.require_virtualized = True
    >>> for build_file in a_build.source_package_release.files:
    ...     removeSecurityProxy(build_file).libraryfile.restricted = False
    >>> mark_archive = getUtility(IPersonSet).getByName('mark').archive

    >>> unused_dep = cprov_archive.addArchiveDependency(
    ...      mark_archive, PackagePublishingPocket.RELEASE,
    ...      getUtility(IComponentSet)['main'])

    >>> commit()
    >>> LaunchpadZopelessLayer.switchDbUser(test_dbuser)
    >>> login(ANONYMOUS)

Now we can see that a build from Celso's PPA will be able to install
dependencies from Mark's PPA, if Mark's PPA has at least one binary
published in hoary/i386, which is not the case.

    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates main restricted universe multiverse
     deb http://ppa.launchpad.dev/cprov/ppa/ubuntu hoary main
    Suite: hoary
    Ogre-component: main
    Archive Purpose: PPA
    Archive Private: False

We will create the required publication in Mark's PPA and try again.

    >>> candidate.destroySelf()
    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> a_builder.is_available
    True

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

    >>> create_binary_publication_for(
    ...    mark_archive, hoary, PackagePublishingStatus.PUBLISHED)

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(candidate)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates main restricted universe multiverse
     deb http://ppa.launchpad.dev/cprov/ppa/ubuntu hoary main
     deb http://ppa.launchpad.dev/mark/ppa/ubuntu hoary main
    Suite: hoary
    Ogre-component: main
    Archive Purpose: PPA
    Archive Private: False

Clean up before continuing:

    >>> candidate.destroySelf()
    >>> a_builder.virtualized = False
    >>> removeSecurityProxy(a_build).archive = ubuntu.main_archive
    >>> commit()

Builddmaster stops before starting to build a denied build.
Since hoary is in development, we are not able to dispatch
builds for post-release pockets:

    >>> candidate = a_build.queueBuild()
    >>> setupBuildQueue(candidate, a_builder)
    >>> last_stub_mail_count = len(stub.test_emails)

Make a build in the updates pocket:

    >>> hoary = hoary_i386.distroseries
    >>> hoary_evo = hoary.getSourcePackage(
    ...    'evolution').currentrelease.sourcepackagerelease
    >>> updates_build = hoary_evo.createBuild(
    ...     distro_arch_series=hoary_i386,
    ...     pocket=PackagePublishingPocket.UPDATES,
    ...     processor=hoary_i386.default_processor,
    ...     archive=hoary_i386.main_archive)
    >>> updates_bqItem = updates_build.queueBuild()

    >>> hoary_i386.distroseries.status.name
    'DEVELOPMENT'
    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(updates_bqItem)
    Traceback (most recent call last):
    ...
    AssertionError: i386 build of evolution 1.0 in ubuntu hoary UPDATES (...) can not be built for pocket UPDATES: invalid pocket due to the series status of hoary.

== Pocket dependencies ==

Change the distroseries status for testing. FROZEN allows building in
all pockets:

    >>> from canonical.launchpad.interfaces import SeriesStatus
    >>> hoary_i386.distroseries.status = SeriesStatus.FROZEN

Now we can start a build in other pockets, and see what archives are
passed to the slave.

A build in the updates pocket:

    >>> a_builder.currentjob.destroySelf()

    >>> bqItem3 = a_build.queueBuild()
    >>> build = getUtility(IBinaryPackageBuildSet).getByQueueEntry(bqItem3)
    >>> removeSecurityProxy(build).pocket = (
    ...     PackagePublishingPocket.UPDATES)
    >>> last_stub_mail_count = len(stub.test_emails)
    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(bqItem3)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main
     deb http://ftpmaster.internal/ubuntu hoary-security main
     deb http://ftpmaster.internal/ubuntu hoary-updates main
    Suite: hoary-updates
    Ogre-component: main
    Archive Purpose: PRIMARY
    Archive Private: False

A build in the proposed pocket:

    >>> a_builder.currentjob.destroySelf()

    >>> bqItem3 = a_build.queueBuild()
    >>> removeSecurityProxy(build).pocket = (
    ...     PackagePublishingPocket.PROPOSED)
    >>> last_stub_mail_count = len(stub.test_emails)
    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(bqItem3)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main
     deb http://ftpmaster.internal/ubuntu hoary-proposed main
     deb http://ftpmaster.internal/ubuntu hoary-security main
     deb http://ftpmaster.internal/ubuntu hoary-updates main
    Suite: hoary-proposed
    Ogre-component: main
    Archive Purpose: PRIMARY
    Archive Private: False

A build in the backports pocket:

    >>> a_builder.currentjob.destroySelf()

    >>> bqItem3 = a_build.queueBuild()
    >>> removeSecurityProxy(build).pocket = (
    ...     PackagePublishingPocket.BACKPORTS)
    >>> last_stub_mail_count = len(stub.test_emails)
    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(bqItem3)
    ensurepresent called, url=...
    ensurepresent called,
        url=http://localhost:58000/3/firefox_0.9.2.orig.tar.gz
    OkSlave BUILDING
    Archives:
     deb http://ftpmaster.internal/ubuntu hoary main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-backports main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-security main restricted universe multiverse
     deb http://ftpmaster.internal/ubuntu hoary-updates main restricted universe multiverse
    Suite: hoary-backports
    Ogre-component: main
    Archive Purpose: PRIMARY
    Archive Private: False

A build in the security pocket:

    >>> a_builder.currentjob.destroySelf()

    >>> bqItem3 = a_build.queueBuild()
    >>> removeSecurityProxy(build).status = (
    ...     BuildStatus.NEEDSBUILD)
    >>> removeSecurityProxy(build).pocket = (
    ...     PackagePublishingPocket.SECURITY)
    >>> last_stub_mail_count = len(stub.test_emails)

The pocket-dependency infrastructure is ready to deal with SECURITY
pocket, however we explicitly skip security builds when dispatching
because Embargoed-Archives and Restricted-UI implementations are not
yet ready.

    >>> removeSecurityProxy(a_builder)._dispatchBuildCandidate(bqItem3)
    Traceback (most recent call last):
    ...
    AssertionError: Soyuz is not yet capable of building SECURITY uploads.

Builds for security pocket are marked as FAILEDTOBUILD inside the
_findBuildCandidate() method, see doc/buildd-dispatching.txt


== Builder Status Handler ==

IBuilder.slaveStatus should return a dict containing the following
items:

 * slave status string:  'BuilderStatus.IDLE'
 * job identifier string: '1-1'
 * job status string: 'BuildStatus.OK' or None
 * logtail (last 1K output of the ongoing build) as xmlrpclib.Binary or None
 * result file list: {'foo.deb', 'foo.changes'} or None
 * dependencies string: 'bar baz zaz' or None

    # Define a helper to print the slave status dict.
    >>> from collections import defaultdict
    >>> def printSlaveStatus(status_dict):
    ...     status_dict = defaultdict(lambda:None, status_dict)
    ...     print (
    ...         "builder_status: %(builder_status)s\n"
    ...         "build_status: %(build_status)s\n"
    ...         "logtail: %(logtail)r\n"
    ...         "filemap: %(filemap)s\n"
    ...         "dependencies: %(dependencies)s\n" % status_dict)

    >>> a_builder.setSlaveForTesting(OkSlave())
    >>> printSlaveStatus(a_builder.slaveStatus())
    builder_status: BuilderStatus.IDLE
    build_status: None
    logtail: None
    filemap: None
    dependencies: None

    >>> a_builder.setSlaveForTesting(BuildingSlave())
    >>> printSlaveStatus(a_builder.slaveStatus())
    builder_status: BuilderStatus.BUILDING
    build_status: None
    logtail: <xmlrpclib.Binary ...>
    filemap: None
    dependencies: None

    >>> a_builder.setSlaveForTesting(WaitingSlave(state='BuildStatus.OK'))
    >>> printSlaveStatus(a_builder.slaveStatus())
    builder_status: BuilderStatus.WAITING
    build_status: BuildStatus.OK
    logtail: None
    filemap: {}
    dependencies: None

    >>> a_builder.setSlaveForTesting(AbortingSlave())
    >>> printSlaveStatus(a_builder.slaveStatus())
    builder_status: BuilderStatus.ABORTING
    build_status: None
    logtail: None
    filemap: None
    dependencies: None

    >>> a_builder.setSlaveForTesting(AbortedSlave())
    >>> printSlaveStatus(a_builder.slaveStatus())
    builder_status: BuilderStatus.ABORTED
    build_status: None
    logtail: None
    filemap: None
    dependencies: None

