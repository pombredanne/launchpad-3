=============
Builder Class
=============

The Builder class represents a slave machine in the build farm. These
slaves are used to execute untrusted code -- for example when building
packages.

There are several builders in the sample data. Let's examine the first.

    >>> from lp.buildmaster.model.builder import Builder
    >>> builder = Builder.get(1)

As expected, it implements IBuilder.

    >>> from canonical.launchpad.webapp.testing import verifyObject
    >>> from lp.buildmaster.interfaces.builder import IBuilder
    >>> verifyObject(IBuilder, builder)
    True

    >>> print builder.name
    bob
    >>> print builder.builderok
    True
    >>> print builder.failnotes
    None

Builders can take on different behaviors depending on the type of build
they are currently processing. Each builder provides an attribute
(current_build_behavior) to which all the build-type specific behavior
for the current build is delegated. In the sample data, bob's current
behavior is dealing with binary packages.

    >>> from zope.security.proxy import isinstance
    >>> from lp.soyuz.model.binarypackagebuildbehavior import (
    ...     BinaryPackageBuildBehavior)
    >>> isinstance(
    ...     builder.current_build_behavior, BinaryPackageBuildBehavior)
    True

A builder has an XML-RPC proxy in the 'slave' attribute, which allows
us to easily call methods on the slave machines.

    >>> s = builder.slave

The base URL of the proxy matches the builder's URL.

    >>> s.urlbase == builder.url
    True


BuilderSet
==========

Builders and groups thereof are managed through a utility, IBuilderSet.

    >>> from zope.component import getUtility
    >>> from lp.buildmaster.interfaces.builder import IBuilderSet
    >>> builderset = getUtility(IBuilderSet)
    >>> verifyObject(IBuilderSet, builderset)
    True

Iterating over a BuilderSet yields all registered builders.

    >>> for b in builderset:
    ...     print b.name
    bob
    frog

count() return the number of builders registered:

    >>> builderset.count()
    2

Builders can be retrieved by name.

    >>> print builderset['bob'].name
    bob
    >>> print builderset['bad']
    None

And also by ID.

    >>> print builderset.get(2).name
    frog
    >>> print builderset.get(100).name
    Traceback (most recent call last):
    ...
    SQLObjectNotFound: Object not found

The 'new' method will create a new builder in the database.

    >>> bnew = builderset.new(1, 'http://dummy.com:8221/', 'dummy',
    ...	                   'Dummy Title', 'eh ?', 1)
    >>> bnew.name
    u'dummy'

'getBuilders' returns builders with the 'active' flag set, ordered by
virtualization status, architecture, then name.

    >>> for b in builderset.getBuilders():
    ...     print b.name
    bob
    dummy
    frog
    >>> login('foo.bar@canonical.com')
    >>> bnew.active = False
    >>> login(ANONYMOUS)
    >>> for b in builderset.getBuilders():
    ...     print b.name
    bob
    frog

'getBuildQueueSizes' returns the number of pending builds for each
Processor/virtualization.

    >>> queue_sizes = builderset.getBuildQueueSizes()
    >>> queue_sizes['nonvirt']['386']
    (1L, datetime.timedelta(0, 60))

There are no 'amd64' build queue entries.

    >>> queue_sizes['nonvirt'].keys()
    [u'386']

The virtualized build queue for 386 is also empty.

    >>> queue_sizes['virt'].keys()
    []

The queue size is not affect by builds target to disabled
archives. Builds for disabled archive are not dispatched as well, this
is an effective manner to hold activity in a specific archive.

We will temporarily disable the ubuntu primary archive.

    >>> login('foo.bar@canonical.com')
    >>> from lp.registry.interfaces.distribution import IDistributionSet
    >>> ubuntu = getUtility(IDistributionSet).getByName('ubuntu')
    >>> ubuntu.main_archive.disable()
    >>> import transaction
    >>> transaction.commit()
    >>> login(ANONYMOUS)

That done, the non-virtualized queue for i386 becomes empty.

    >>> queue_sizes = builderset.getBuildQueueSizes()
    >>> queue_sizes['nonvirt'].keys()
    []

Let's re-enable the ubuntu primary archive.

    >>> login('foo.bar@canonical.com')
    >>> ubuntu.main_archive.enable()
    >>> transaction.commit()
    >>> login(ANONYMOUS)

The build for the ubuntu primary archive shows up again.

    >>> queue_sizes = builderset.getBuildQueueSizes()
    >>> queue_sizes['nonvirt']['386']
    (1L, datetime.timedelta(0, 60))

All job types are included. If we create a recipe build job, it will
show up in the calculated queue size.

    >>> recipe_bq = factory.makeSourcePackageRecipeBuildJob()
    >>> # XXX wgrant 20100625 bug=598397: The factory erroneously creates
    >>> # architecture-independent jobs.
    >>> print recipe_bq.processor
    None

    >>> from lp.soyuz.interfaces.processor import IProcessorFamilySet
    >>> i386_family = getUtility(IProcessorFamilySet).getByName('x86')
    >>> recipe_bq.processor = i386_family.processors[0]
    
    >>> transaction.commit()
    >>> queue_sizes = builderset.getBuildQueueSizes()
    >>> print queue_sizes['virt']['386']
    (1L, datetime.timedelta(0, 64))


Resuming buildd slaves
======================

Virtual slaves are resumed using a command specified in the
configuration profile. Production configuration uses a SSH trigger
account accessed via a private key available in the builddmaster
machine (which used ftpmaster configuration profile) as in:

{{{
ssh ~/.ssh/ppa-reset-key ppa@%(vm_host)s
}}}

The test configuration uses a fake command that can be performed in
development machine and allow us to tests the important features used
in production, as 'vm_host' variable replacement.

    >>> from canonical.config import config
    >>> config.builddmaster.vm_resume_command
    'echo %(vm_host)s'

Before performing the command, it checks if the builder is indeed
virtual and raises CannotResumeHost if it isn't.

    >>> bob = getUtility(IBuilderSet)['bob']
    >>> bob.resumeSlaveHost()
    Traceback (most recent call last):
    ...
    CannotResumeHost: Builder is not virtualized.

For testing purposes resumeSlaveHost returns the stdout and stderr
buffer resulted from the command.

    >>> frog = getUtility(IBuilderSet)['frog']
    >>> out, err = frog.resumeSlaveHost()
    >>> print out.strip()
    localhost-host.ppa

If the specified command fails, resumeSlaveHost also raises
CannotResumeHost exception with the results stdout and stderr.

    # The command must have a vm_host dict key and when executed,
    # have a returncode that is not 0.
    >>> vm_resume_command = """
    ...     [builddmaster]
    ...     vm_resume_command: test "%(vm_host)s = 'false'"
    ...     """
    >>> config.push('vm_resume_command', vm_resume_command)
    >>> frog.resumeSlaveHost()
    Traceback (most recent call last):
    ...
    CannotResumeHost: Resuming failed:
    OUT:
    <BLANKLINE>
    ERR:
    <BLANKLINE>

Restore default value for resume command.

    >>> config_data = config.pop('vm_resume_command')


Slave architecture checks
=========================

Builder.checkSlaveArchitecture() asks the slave for its version and tries
to match it against a DistroArchSeries with a ProcessorFamily containing
the Builder's Processor. If it fails, it will raise an exception so the
builder can be marked as failed.

A fictitious i386 variant is rejected, since there are no DASes with that
tag.

    >>> from lp.soyuz.tests.soyuzbuilddhelpers import OkSlave
    >>> bob.setSlaveForTesting(OkSlave('i387'))
    >>> bob.checkSlaveArchitecture()
    Traceback (most recent call last):
    ...
    BuildDaemonError: Bad slave architecture tag: i387 (registered family: x86)

hppa isn't in the x86 family, so it too is rejected.

    >>> from lp.soyuz.tests.soyuzbuilddhelpers import OkSlave
    >>> bob.setSlaveForTesting(OkSlave('hppa'))
    >>> bob.checkSlaveArchitecture()
    Traceback (most recent call last):
    ...
    BuildDaemonError: Bad slave architecture tag: hppa (registered family: x86)

But i386, a real x86 variant, passes without objection.

    >>> from lp.soyuz.tests.soyuzbuilddhelpers import OkSlave
    >>> bob.setSlaveForTesting(OkSlave('i386'))
    >>> bob.checkSlaveArchitecture()


Rescuing lost slaves
====================

Builder.rescueIfLost() checks the build ID reported in the slave status
against the database. If it isn't building what we think it should be,
the current build will be aborted and the slave cleaned in preparation
for a new task. The decision about the slave's correctness is left up
to IBuildFarmJobBehavior.verifySlaveBuildCookie -- for these examples we
will use a special behavior that just checks if the cookie reads 'good'.

    >>> import logging
    >>> from lp.buildmaster.interfaces.builder import CorruptBuildCookie
    >>> from lp.soyuz.tests.soyuzbuilddhelpers import (
    ...    BuildingSlave, MockBuilder, OkSlave, WaitingSlave)

    >>> class TestBuildBehavior:
    ...     def verifySlaveBuildCookie(self, cookie):
    ...         if cookie != 'good':
    ...             raise CorruptBuildCookie('Bad value')

    >>> def rescue_slave_if_lost(slave):
    ...     builder = MockBuilder('mock', slave, TestBuildBehavior())
    ...     builder.rescueIfLost(logging.getLogger())

An idle slave is not rescued.

    >>> rescue_slave_if_lost(OkSlave())

Slaves building or having built the correct build are not rescued
either.

    >>> rescue_slave_if_lost(BuildingSlave(build_id='good'))
    >>> rescue_slave_if_lost(WaitingSlave(build_id='good'))

But if a slave is building the wrong ID, it is declared lost and
an abort is attempted. MockSlave prints out a message when it is aborted
or cleaned.

    >>> rescue_slave_if_lost(BuildingSlave(build_id='bad'))
    Aborting slave
    WARNING:root:Builder 'mock' rescued from 'bad': 'Bad value'

Slaves having completed an incorrect build are also declared lost,
but there's no need to abort a completed build. Such builders are
instead simply cleaned, ready for the next build.

    >>> rescue_slave_if_lost(WaitingSlave(build_id='bad'))
    Cleaning slave
    WARNING:root:Builder 'mock' rescued from 'bad': 'Bad value'

